# OpenStack Service Configuration

There are two ways to configure OpenStack's services.

The first and most common method is to modify Kolla-Ansible's [globals file](/openstack-kolla-globals.html).
While the options directly supported by Kolla-Ansible and its globals file are extensive, they
aren't exhaustive. Some configurations, such as Ceph's keyring files, can't be done this way.

The second approach involves writing your own configuration files. They'll be merged with those
generated by Kolla-Ansible and overwrite any values it had previously generated. The files are
expected to have particular names and exist in specific directories to function correctly. When
deploying a basic LVM cluster, no custom files need to be written.

To fine-tune the OpenStack services, create a `config/` directory.

This directory will be used with the `voithos openstack kolla-ansible` command,
specified by the `--config-dir` option.

Ceph-backed clusters should have already created some files here, as specified in the
[OpenStack Ceph setup guide](/openstack-ceph.html).


---


## config/elasticsearch/elasticsearch.yml

## Field data cache
Field data cache is memory used while sorting or computing aggregation on a field.
Define the percentage of node heap size used by field data cache. Default value is
40%.

```
# config/elasticseach/elasticsearch.yml
---
indices.fielddata.cache.size: 10%
```


## config/ceilometer/polling.yaml

### Metric polling

This configuration is strictly required for the Arcus self-service portal.
The `polling.yaml` file defines the metrics required by the Arcus self-service portal.

```
# config/ceilometer/polling.yaml
---
sources:
    - name: some_pollsters
      interval: 300
      meters:
        - cpu
        - memory.usage
    - name: some_pollsters
      interval: 3600
      meters:
        - volume.size
```


## config/ceilometer/pipeline.yaml

### Pipeline

This configuration is strictly required for the Arcus self-service portal.

The `pipeline.yaml` file configures Ceilometer to use custom metering policies 
"`metering-policy-usage`" and "`metering-policy-utilization`". These two archive policies 
will be created after deployment. "`metering-policy-usage`" will act as archive-policy-rule for  
arcus usage metrics like vcpus, memory and volume.size to get their measures with one hour 
granularity. Whereas "`metering-policy-utilization` will act as archive-policy-rule for
arcus utilization metrics like cpu and memory.usage to get their measures with five minutes 
granularity.

```
# config/ceilometer/pipeline.yaml
---
sources:
    - name: meter_source_usage
      meters:
          - "vcpus"
          - "memory"
          - "volume.size"
      sinks:
          - meter_sink_usage
    - name: meter_source_utilization
      meters:
          - "cpu"
          - "memory.usage"
      sinks:
          - meter_sink_utilization
sinks:
    - name: meter_sink_usage
      publishers:
          - gnocchi://?archive_policy=metering-policy-usage
    - name: meter_sink_utilization
      publishers:
          - gnocchi://?archive_policy=metering-policy-utilization


```


## config/nova/nova-compute.conf

### Resource Reservations

To prevent OpenStack from starving your hosts of resources, you should set aside some. This is
particularly important in hyperconverged deployments.

```
# config/nova/nova-compute.conf

[DEFAULT]
reserved_host_memory_mb = 16384
reserved_host_cpus = 4
reserved_host_disk_mb = 200000
```

### Build retries

When using iSCSI you usually need this. With ceph it isn't required.

```
# config/nova/nova-compute.conf

[DEFAULT]
# block_device_allocate_retries_interval default is 3
block_device_allocate_retries_interval = 30
# block_device_allocate_retries default is 60
block_device_allocate_retries = 180
```

### Overcommit ratios

We don't suggest overcommitting RAM, else you invite the OOM killer to your environment. We find
a 5:1 CPU overcommit ratio to be safe in production.

```
# config/nova/nova-compute.conf

[DEFAULT]
cpu_allocation_ratio = 5
ram_allocation_ratio = 1
```

## config/nova/nova-api.conf

### Disable Ephemral Disk Usage

Add these configs to prevent Openstack from using hypervisor's root disk.
It won't allow user to create instance using ephemeral disk.

```
# config/nova/nova-api.conf

[DEFAULT]
max_local_block_devices = 0
```


## config/neutron/neutron.conf

### Enable MTU 9000

```
[DEFAULT]
global_physnet_mtu = 9000
```

## config/neutron/ml2_conf.ini

### Enable VLANs

Neutron requires that the permitted VLAN range be defined on the physical provider network physnet1

```
#  config/neutron/ml2_conf.in

[ml2_type_vlan]
network_vlan_ranges = physnet1:1:4094
```


## config/neutron/dnsmasq.conf

### Network Associated Search Domain
Put entries to associate network with search domains along with default log-facility path.
Those networks will use search domains associated with them in this file instead of default
search domain.
```
# config/neutron/dnsmasq.conf
log-facility=/var/log/kolla/neutron/dnsmasq.log
domain=<search-domain-name-1>, <network-address-1>
.
.
.
domain=<search-domain-name-n>, <network-address-n>
```


## config/neutron/neutron-dhcp-agent.conf

### Default Search Domain
Add dns_name config in order to set deafult search domain. This search domain will be used by
all networks except ones that are defined in config/neutron/dnsmasq.conf.
```
# config/neutron/neutron-dhcp-agent.conf
# Don't forget to add "." at the end of serach domain name.
[DEFAULT]
dns_domain = <search-domain>.
```


## config/prometheus/prometheus.yml
Default scrape interval is 60 seconds. That means prometheus server will fetch monitoring data
from it's exporters after every one minute. It might put stress on server resources depending
on your servers specs. In order to use higher scrape interval we can configure
`prometheus.yml` before deployment. Replace values enclosed in `<>` with actual values.
Consult inventory file for checking what hosts belong to monitoring, control and networking.
All exporters are enabled by default. If any of the exporter is disabled in globals file,
don't put it's configs under `scrape_configs`. Elasticsearch exporter is enabled when elasticsearch
is enabled. Don't put it's configs under `scrape_configs` if elasticsearch isn't enabled.

```
# config/prometheus/prometheus.yml
global:
  scrape_interval: <scrape-interval>s
  scrape_timeout: 10s
  evaluation_interval: 15s
  external_labels:
    monitor: 'kolla'

scrape_configs:
  - job_name: prometheus
    static_configs:
      - targets:
        - '<monitoring host 1 api ip address>:9091'
        - '<monitoring host n api ip address>:9091'

  - job_name: node
    static_configs:
      - targets:
        # All openstack nodes participate in this job
        - '<openstack host 1 api ip address>:9100'
        - '<openstack host n api ip address>:9100'

  - job_name: mysqld
    static_configs:
      - targets:
        - '<control host 1 api ip address>:9104'
        - '<control host n api ip address>:9104'

  - job_name: haproxy
    static_configs:
      - targets:
        - '<network host 1 api ip address>:9101'
        - '<network host n api ip address>:9101'

  - job_name: memcached
    static_configs:
      - targets:
        - '<control host 1 api ip address>:9150'
        - '<control host n api ip address>:9150'

  - job_name: cadvisor
    static_configs:
      # All openstack nodes participate in this job
      - targets:
        - '<openstack host 1 api ip address>:18080'
        - '<openstack host n api ip address>:18080'


  - job_name: openstack_exporter
    honor_labels: true
    static_configs:
      - targets:
        - '<monitoring host 1 api ip address>:9198'
        - '<monitoring host n api ip address>:9198'

  - job_name: elasticsearch_exporter
    static_configs:
      - targets:
        - '<control host 1 api ip address>:9108'
        - '<control host n api ip address>:9108'

  - job_name: ceph_mgr_exporter
    # Add this job only if prometheus_ceph_mgr_exporter is enabled in globals file.
    honor_labels: true
    static_configs:
      - targets:
        - '<monitoring host 1 api ip address>:9283'
        - '<monitoring host n api ip address>:9283'

alerting:
  alertmanagers:
  - static_configs:
    - targets:
        - '<control host 1 api ip address>:9093'
        - '<control host n api ip address>:9093'
```

## config/keystone/keystone.conf

### Token Timeouts

Often our users upload HUGE glance images. Extending the token timeout helps with this.

```
# config/keystone/keystone.conf

[token]
expiration = 7200
```

## config/octavia.conf
These configs are required for creating loadbalancers using bootable volumes.
```yaml
[controller_worker]
volume_driver = volume_cinder_driver

[cinder]
region_name = RegionOne
endpoint_type = internal
# Size of bootable volume to create loadbalancer. 2 will work
volume_size = <volume-size>
# It's fine if this volume type hasn't been created yet.
volume_type = <volume-type-name>
```


### LDAP Configuration

Keystone can optionally use LDAP as an authentication backend. Breqwatr only supports LDAP using
Microsoft Active Directory.

- [OpenStack LDAP documentation](https://docs.openstack.org/keystone/pike/admin/identity-integrate-with-ldap.html)
- [Keystone config reference](https://docs.openstack.org/ocata/config-reference/identity/samples/keystone.conf.html)
- [Kolla-Ansible's keystone.conf Jinja2 template](https://github.com/openstack/kolla-ansible/blob/master/ansible/roles/keystone/templates/keystone.conf.j2)

By creating the domains directory, Kolla-Ansible's Jinja2 tempalte will automatically enable
`domain_specific_drivers_enabled`, so it doesn't need to set. The `[identity] driver` key still
needs to be defined.


## Keystone Domain-specific path: config/keystone/domains/keystone.DOMAIN\_NAME.conf

### Domain Creation

In the OpenStack cli, create the domain

```bash
openstack domain create <name>
```

### Domain Configuration

When using LDAP with Keystone, each domain's data should be configured in its own file.

Where the target domain to be "example.com", you would create a file named
`config/keystone/domains/keystone.example.com.conf` to define the domain details.


```ini
# config/keystone/domains/keystone.DOMAIN_NAME.conf

[identity]
driver = ldap


[ldap]

# Multiple LDAP URLs may be specified as a comma separated string
url = ldap://localhost

# Service account credentials
user = dc=Manager,dc=example,dc=org
password = samplepassword

# The default LDAP server suffix to use, if a DN is not defined via either
suffix = dc=example,dc=org

# Define the search trees for LDAP
user_tree_dn = ou=Users,dc=example,dc=org
group_tree_dn = ou=Groups,dc=example,dc=org

# Use a group to determine which users have access
#   including :1.2.840.113556.1.4.1941: enables nested AD group search
user_filter = (memberOf:1.2.840.113556.1.4.1941:=cn=grp-openstack,CN=Users,DC=ad,DC=local)
# If you included the nested search filter, also consider setting (default False):
group_ad_nesting = True

# Active Directory uses the following object classes & attributes
user_objectclass = person
group_objectclass = group
user_name_attribute = sAMAccountName
user_id_attribute = cn
user_mail_attribute = mail
user_pass_attribute = userPassword
user_enabled_attribute = userAccountControl
user_enabled_mask = 2
user_enabled_invert = false
user_enabled_default = 512
group_id_attribute = cn
group_name_attribute = ou
group_member_attribute = member
group_desc_attribute = description

# In big domains with nested OUs you may also need:
chase_referrals = false
query_scope = sub
page_size = 999
pool_size=100

# debug levels of 0,255, and 4095 are common, with 4095 the most verbose
debug_level = 0
```

